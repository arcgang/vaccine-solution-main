---
title: Order management and optimization demo
description: Order management and optimization demonstration
--- 

The vaccine order fullfilment is a complex problem to address as we need to optimize the shipment plan according to the demand and the contraints. A deep dive to the O/R problem is described in [this note](/solution/orderms/voro-solution/). In this scenario we want to demostrate the order entry with an event-driven microservice, and the integration with an event driven linear programming stateful function. The scenario addresses the following features:

* Create Order(s) using a simple user interface as an order manager will do after interacting with the country health care request.
* Validate the [transactional outbox pattern](https://ibm-cloud-architecture.github.io/refarch-eda/patterns/intro/#transactional-outbox) works to get OrderCreated, OrderUpdated or OrderCancelled Events created into a dedicated table in Postgresql
* Validate how [Debezium Change Data Capture](https://debezium.io/documentation/reference/1.4/connectors/postgresql.html) for Postgresql as a Kafka connector, produces OrderEvents from the table to Kafka `orders` topic.
* Integrate with the [Shipment plan optimization]() to get the updated shipment plan as events.

## Components involved in this use case

* [Vaccine Order Service](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg)
* Postgres DB
* Debezium for change data capture on the outbox table
* Kafka or [event streams](https://ibm.github.io/event-streams/)
* [Vaccine Order Reefer Optimization Service](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer)

 ![](../../solution/orderms/images/vaccine-order-1.png)

*The blockchain integration is not done yet*

To have a deeper understanding of the order service read [this overview section](/solution/orderms/#overview). For the Vaccine order reefer optimization service [the design article](/design/voro/) details the optimization approach, while the [solution article](/solution/orderms/voro-solution/#overview) goes over the service implementation details.

## Run to OpenShift 

### Pre-requisites

* You need:
    * docker and docker compose on your laptop to run locally or build image
    * Java 11
    * git client 
    * Get access to an OpenShift cluster with Cloud Pak for integration and event streams installed.
1. Create the following artifacts in the `eventstreams` namespace on your OpenShift cluster:

   1. Create an EventStreams instance _(via the [Event Streams Custom Resource](https://ibm.github.io/event-streams/installing/installing/#install-an-event-streams-instance))_.
   2. Create a [Kafka User with SCRAM-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Reefer Simulator](https://github.com/ibm-cloud-architecture/vaccine-reefer-simulator#application-deployment).
   3. Create a [Kafka User with TLS-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Monitoring Agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent#create-a-tls-user).
   4. Create two [Kafka Topics](https://ibm.github.io/event-streams/getting-started/creating-topics/). This tutorial will assume the names of `vaccine.shipment.plans` respectively.

1. Create a new project named `vaccineorder` that will be used for the deployment of all other components.
1. [OpenShift CLI](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli) on your local environment.
1. [jq](https://stedolan.github.io/jq/) on your local environment.
1. Use a Terminal and the oc cli. If you want to access the code source you can clone the two main repositories of this solution:

 ```shell
 git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg
 git clone https://github.com/ibm-cloud-architecture/vaccine-order-optimization-pg
 ```

### Deploy the Vaccine Order Service

This microservice is built using maven and Quarkus extensions. In this current main project we have 
We have already pushed the last version of this service on dockerhub, if you do not want to build it. 

1. Ensure you are working inside the correct project via the following `oc` command:

  ```shell
  export PROJECT_NAME=vaccineorder
  oc project ${PROJECT_NAME}
  ```

1. Export the value of your Event Streams cluster name into an environment variable:

  ```shell
  export CLUSTER_NAME=eda-dev
  export EVENTSTREAMS_NS=eventstreams
  ```
   * To check what the name of your Event Streams cluster name is do:
   ```shell
   $ oc get eventstreams -n ${EVENTSTREAMS_NS}
   NAME           STATUS
   development    Ready
   ```
1. Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:

  ```shell
  # Define environement variables
  SERVICE_ACCOUNT_NAME=postgres-sa
  DEPLOYMENT_NAME=postgres
  SERVICE_NAME=postgres
  DOCKER_IMAGE=docker.io/postgres:11.6-alpine
  POSTGRES_PASSWORD=adifficultpasswordtoguess

  oc create serviceaccount ${SERVICE_ACCOUNT_NAME}
  oc adm policy add-scc-to-user anyuid -n ${PROJECT_NAME} -z ${SERVICE_ACCOUNT_NAME}
  oc create deployment ${DEPLOYMENT_NAME} --image=${DOCKER_IMAGE}
  oc set serviceaccount deployment/${DEPLOYMENT_NAME} ${SERVICE_ACCOUNT_NAME}
  oc patch deployment ${DEPLOYMENT_NAME} --type="json" -p='[{"op":"add", "path":"/spec/template/spec/containers/0/args", "value":[]},{"op":"add", "path":"/spec/template/spec/containers/0/args/-", "value":"-c"},{"op":"add", "path":"/spec/template/spec/containers/0/args/-", "value":"wal_level=logical"} ]'
  oc set env deployment ${DEPLOYMENT_NAME} POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
  oc expose deployment ${DEPLOYMENT_NAME} --port 5432 --name ${SERVICE_NAME}
  ```

1. Get Kafka TLS user name to access Event Streams bootstrap using the external route. export this user name in KAFKA_USER environment variable:

  ```shell
  oc get kafkausers -n $EVENTSTREAMS_NS 
  # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION
  # app-scram                           eda-dev   scram-sha-512    simple
  # app-tls                             eda-dev   tls              simple
  export KAFKA_USER=app-tls
  ```
1. Copy Kafka TLS user certificate to target project:

  ```shell
  oc get secret ${KAFKA_USER} -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.namespace="'${PROJECT_NAME}'"' | oc apply -f -
  ```
1. Get Kafka bootstrap server URL within

   ```shell
   export KAFKA_BROKERS=$(oc get route -n ${EVENTSTREAMS_NS} ${CLUSTER_NAME}-kafka-bootstrap -o jsonpath="{.status.ingress[0].host}:443")
   ```

 1. Copy TLS server CA certificate from eventstreams project to local project with the command:

   ```shell
   oc get secret ${CLUSTER_NAME}-cluster-ca-cert -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.name="kafka-cluster-ca-cert"' |jq -r '.metadata.namespace="'${PROJECT_NAME}'"' | oc apply -f -
   ``` 

1. Define environment variables for the order service with config map and secrets:

```shell
# config map
oc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/configmap.yaml
# Secrets
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: vaccine-order-secrets
stringData:
  KAFKA_USER: ${KAFKA_USER}
  KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_USER}
  QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}
  QUARKUS_DATASOURCE_USERNAME: postgres
```

* Deploy the app. If you have cloned the repository and have java you can use the following command:

 ```shell
 ./mvnw clean package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests
 ```

 If not, you can deploy the application using the deployment config as we have pushed the ibmcase/vaccineorders image to dockerhub:

 ```shell
oc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/deploymentconfig.yaml

 ```

### Deploy Debezium CDC connector

The Event Streams product documentation goes over the tasks to be done, but to summarize them, here are the actions done to deploy the postgres debezium connector:

* Start a Kafka connector cluster: We use the custom resource called `KafkaConnectS2I`, which one instance represents a Kafka connect cluster. Each connector is represented by another custom resource called KafkaConnector. Kafka connect needs a user access to the Brokers. We can use the TLS user as previously done. 
  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.
  * Modify the bootstrap server and the TLS user to be used. Ensure the user can access any topics.
  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n eventstreams`
  * Validate it via: 

   ```shell
   oc get kafkaconnects2i -n eventstreams
   oc describe kafkaconnects2i connect-cluster -n eventstreams
   ```
* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. 

  ```
  ├── my-plugins
  │   └── debezium-connector
  │       ├── CHANGELOG.md
  │       ├── CONTRIBUTE.md
  │       ├── COPYRIGHT.txt
  │       ├── LICENSE-3rd-PARTIES.txt
  │       ├── LICENSE.txt
  │       ├── README.md
  │       ├── README_ZH.md
  │       ├── debezium-api-1.4.0.Final.jar
  │       ├── debezium-connector-postgres-1.4.0.Final.jar
  │       ├── debezium-core-1.4.0.Final.jar
  │       ├── failureaccess-1.0.1.jar
  │       ├── guava-30.0-jre.jar
  │       ├── postgresql-42.2.14.jar
  │       └── protobuf-java-3.8.0.jar
  └── pg-connector.yaml
  ```

* Deploy the connector configuration:

  ```shell
  oc start-build connect-cluster-connect --from-dir ./my-plugins/
  oc get builds
  # NAME                        TYPE     FROM             STATUS    STARTED          DURATION
  # connect-cluster-connect-3   Source   Binary@f639186   Running   19 seconds ago   
  # once build is completed... wait to get the 3 kafka connect workers ready
  oc get pods -w
  ```
* Start the connector: `oc apply -f pg-connector.yaml`
* Verify it is running: `oc describe kafkaconnector pg-connector` 

### Deploy the Optimization Service

## Scenario script

Once the solution is up and running in your target deployment environment (local or OpenShift) execute the following steps to present an end to end demonstration.


### Validate existing vaccine lot shipment plan

### Create an order from the user interface of the Order Service. 

### See how the shipment plan is modified

### Validate the blockchain records

### Present the events created

