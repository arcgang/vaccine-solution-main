{"componentChunkName":"component---src-pages-analyze-ws-ml-dev-mdx","path":"/analyze/ws-ml-dev/","result":{"pageContext":{"frontmatter":{"title":"Watson Studio - developing the predictive model","description":"Watson Studio - developing the predictive model"},"relativePagePath":"/analyze/ws-ml-dev.mdx","titleType":"append","MdxNode":{"id":"063d6aed-a078-568c-8b14-ba8b31c74b80","children":[],"parent":"9d7f4037-d759-5fae-9e59-0c1ab953353e","internal":{"content":"---\ntitle: Watson Studio - developing the predictive model\ndescription:  Watson Studio - developing the predictive model\n---\n\nCloud Pak for data integrates [Watson Studio](https://www.ibm.com/cloud/watson-studio) to develop manchine learning models and do feature engineering. In this chapter we are using two approaches to develop the model, one using [AutoAI](https://www.ibm.com/cloud/watson-studio/autoai) and one use [notebook](#notebook).\n\nThe Data Steward has prepared a dataset by joining different datasources, but he did not pay attention to the column semantic for building a machine learning. So the Data Scientist will start to review and adapt the data.\n\n## Data analysis and refinery\n\nThe data scientist wants to do at least two things on the current data set: remove unnecessary features (the longitude and lattitude will not have any value to assess the sensor anomaly), and transform the problem to be a classification problem by adding a label column. \n\nNote that we could have model the problem using unsupervised learning and identify anomaly with clustering or anomaly detection. This will be done in the future to present a more realistic approach to this classical industrial problem.\n\nFor that, we use the `Data Refinery` capability of Cloud Pak for Data:\n\n![Access to Refinery](images/access-refine.png)\n\nand then clean and shape the data to prepare for the model. For example remove columns like latitude, longitude, timestamp, _id, telemetry_id:\n\n![Remove columns](images/remove-column.png)\n\nTo define a new column to be used as label, we use the `Operation` tab to add `Conditional replace` to apply some basic logic on the sensor columns by using one thresholds: test the value of the CO2 to be greater than or equal to threshold (22):\n\n![Define condition](images/a_anomally_def_1.png)\n\n\nWe add the new label (Issue =1, NoIssue=0) in the Issue Flag column:\n\nWhich translates to something like:\n\"Replaced values for Issue: carbon_dioxide_level where value is greater than or equal to 22 as 1. Replaced all remaining values with 0.\"\n\n\nOnce the label column is added, and any new derived data are added, we can start a `refinery job`, that will create a new dataset in our project:\n\n![Refinery job](images/a_dr_job_1.png)\n\n\n## AutoAI\n\nAutoAI uses data to automatically select the best supervised algorithm to determine the best classification or regression models with optimized hyper parameters.\n\nWithin a project, we can add an `Auto AI Experiment`:\n\n![Auto AI Experiment](images/autoai-experiment.png)\n\nThen specify a name and server configuration:\n\n![Auto AI Experiment name](images/autoai-experiment-2.png)\n\nAdd a existing data source (the one prepared by the refinery job), and then specify the column to use for prediction (Issue Flag column):\n\n![](images/a_autoai_start_1.png)\n\nThen execute the prediction experiment.\n\nAutoAI will do different steps, split the data, prepare data, and then select model or algorithm that may better address the problem. For classification model, it will select among 30 potential candidates: decision tree, random forest, LGBM, XGBoost... Each algorithm selection will generate a pipeline which will be scored to present the most accurate one:\n\n![autoai](images/autoai-1.png)\n\n\nEach model pipeline is scored for a variety of metrics and then ranked. The default ranking metric for binary classification models is the area under the ROC curve, for multi-class classification models is accuracy, and for for regression models is the root mean-squared error (RMSE). The highest-ranked pipelines are displayed in a leaderboard, so you can view more information about them. The leaderboard also provides the option to save select model pipelines after reviewing them.\n\n![autoai](images/a_Autoai_2.png)\n\nThe resulting experiments are ranked.\n\n![autoai](images/autoai-2.png)\n\nWhen models or pipelines are created, we can see the details of each model, to see how they performed.\n\n![model-evaluation](images/a_model_evaluation_1.png)\n\n\nThe confusion matrix for the experiment ranked #1:\n\n\n![confusion-matrix](images/a_Confution_Matrix_1.png)\n\n\nThe tool offers nice capabilities like the feature transformation:\n\n![feature-transform](images/a_Feature_Transformation_1.png)\n\nAnd the feature importance ranking, which helps to assess what are the features that impact the classification.\n\n![feature-importance](images/a_Feature_Importance_1.png)\n\nOnce the model is saved, it can be added to a Catalog, and then being deployed.\n\n![saved-models](images/a_model_publishcatalog_promote_1.png)\n\n### Model Deployment\n\nOnce a model is created is promoted to a \"space\".\n\n![model-to-promote](images/model-to-promote.png)\n\nA **space** contains an overview of deployment status, the deployable assets, associated input and output data, and the associated environments.\n\nTo access the deployment space, use the main left menu under `Analyze -> Analytics deployment spaces`.\n\n![spaces](images/spaces.png)\n\nAnd under a given space we can see the assets and deployments in this space:\n\n![space](images/aspace.png)\n\nOn the Assets page, you can view the assets in the deployment space (Here we have our AutoAI experimental model deployed). You can see how many deployments each asset has and whether the asset is configured for performance monitoring. The following figure displays the deployed model\n\n![service](images/online-service.png)\n\nAnd going into the deployed model view, we can see the API end point:\n\n![api](images/model-api.png)\n\nAnd even test the prediction from the user interface.\n\n![test](images/test-api.png)\n\nA next step is to infuse this model into the scoring application...\n\n## Notebook\n\nIn this section, we are developing a notebook on the telemetries dataset. We have already a project, and defined a data set from [the data collection step](../collect/cp4d-collect-data.md), so we need to add a notebook. For that, in the project view, use `Add to project` ...\n\n![add-to-project](images/add-to-project.png)\n\nand select the Notebook, \n\n![add-asset](images/add-asset.png)\n\nspecify a name and select python 3.6 runtime...\n\n![notebook](images/notebook-1.png)\n\n\nAdd a cell to access pandas, and maplablib in the first cell, and then get the code for accessing the dataset, by using the top right button:\n\n![first cells](images/notebook-2.png)\n\n\nTo Be Continued...\n","type":"Mdx","contentDigest":"4b3a91f3648024b282f6a02e04624358","counter":264,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Watson Studio - developing the predictive model","description":"Watson Studio - developing the predictive model"},"exports":{},"rawBody":"---\ntitle: Watson Studio - developing the predictive model\ndescription:  Watson Studio - developing the predictive model\n---\n\nCloud Pak for data integrates [Watson Studio](https://www.ibm.com/cloud/watson-studio) to develop manchine learning models and do feature engineering. In this chapter we are using two approaches to develop the model, one using [AutoAI](https://www.ibm.com/cloud/watson-studio/autoai) and one use [notebook](#notebook).\n\nThe Data Steward has prepared a dataset by joining different datasources, but he did not pay attention to the column semantic for building a machine learning. So the Data Scientist will start to review and adapt the data.\n\n## Data analysis and refinery\n\nThe data scientist wants to do at least two things on the current data set: remove unnecessary features (the longitude and lattitude will not have any value to assess the sensor anomaly), and transform the problem to be a classification problem by adding a label column. \n\nNote that we could have model the problem using unsupervised learning and identify anomaly with clustering or anomaly detection. This will be done in the future to present a more realistic approach to this classical industrial problem.\n\nFor that, we use the `Data Refinery` capability of Cloud Pak for Data:\n\n![Access to Refinery](images/access-refine.png)\n\nand then clean and shape the data to prepare for the model. For example remove columns like latitude, longitude, timestamp, _id, telemetry_id:\n\n![Remove columns](images/remove-column.png)\n\nTo define a new column to be used as label, we use the `Operation` tab to add `Conditional replace` to apply some basic logic on the sensor columns by using one thresholds: test the value of the CO2 to be greater than or equal to threshold (22):\n\n![Define condition](images/a_anomally_def_1.png)\n\n\nWe add the new label (Issue =1, NoIssue=0) in the Issue Flag column:\n\nWhich translates to something like:\n\"Replaced values for Issue: carbon_dioxide_level where value is greater than or equal to 22 as 1. Replaced all remaining values with 0.\"\n\n\nOnce the label column is added, and any new derived data are added, we can start a `refinery job`, that will create a new dataset in our project:\n\n![Refinery job](images/a_dr_job_1.png)\n\n\n## AutoAI\n\nAutoAI uses data to automatically select the best supervised algorithm to determine the best classification or regression models with optimized hyper parameters.\n\nWithin a project, we can add an `Auto AI Experiment`:\n\n![Auto AI Experiment](images/autoai-experiment.png)\n\nThen specify a name and server configuration:\n\n![Auto AI Experiment name](images/autoai-experiment-2.png)\n\nAdd a existing data source (the one prepared by the refinery job), and then specify the column to use for prediction (Issue Flag column):\n\n![](images/a_autoai_start_1.png)\n\nThen execute the prediction experiment.\n\nAutoAI will do different steps, split the data, prepare data, and then select model or algorithm that may better address the problem. For classification model, it will select among 30 potential candidates: decision tree, random forest, LGBM, XGBoost... Each algorithm selection will generate a pipeline which will be scored to present the most accurate one:\n\n![autoai](images/autoai-1.png)\n\n\nEach model pipeline is scored for a variety of metrics and then ranked. The default ranking metric for binary classification models is the area under the ROC curve, for multi-class classification models is accuracy, and for for regression models is the root mean-squared error (RMSE). The highest-ranked pipelines are displayed in a leaderboard, so you can view more information about them. The leaderboard also provides the option to save select model pipelines after reviewing them.\n\n![autoai](images/a_Autoai_2.png)\n\nThe resulting experiments are ranked.\n\n![autoai](images/autoai-2.png)\n\nWhen models or pipelines are created, we can see the details of each model, to see how they performed.\n\n![model-evaluation](images/a_model_evaluation_1.png)\n\n\nThe confusion matrix for the experiment ranked #1:\n\n\n![confusion-matrix](images/a_Confution_Matrix_1.png)\n\n\nThe tool offers nice capabilities like the feature transformation:\n\n![feature-transform](images/a_Feature_Transformation_1.png)\n\nAnd the feature importance ranking, which helps to assess what are the features that impact the classification.\n\n![feature-importance](images/a_Feature_Importance_1.png)\n\nOnce the model is saved, it can be added to a Catalog, and then being deployed.\n\n![saved-models](images/a_model_publishcatalog_promote_1.png)\n\n### Model Deployment\n\nOnce a model is created is promoted to a \"space\".\n\n![model-to-promote](images/model-to-promote.png)\n\nA **space** contains an overview of deployment status, the deployable assets, associated input and output data, and the associated environments.\n\nTo access the deployment space, use the main left menu under `Analyze -> Analytics deployment spaces`.\n\n![spaces](images/spaces.png)\n\nAnd under a given space we can see the assets and deployments in this space:\n\n![space](images/aspace.png)\n\nOn the Assets page, you can view the assets in the deployment space (Here we have our AutoAI experimental model deployed). You can see how many deployments each asset has and whether the asset is configured for performance monitoring. The following figure displays the deployed model\n\n![service](images/online-service.png)\n\nAnd going into the deployed model view, we can see the API end point:\n\n![api](images/model-api.png)\n\nAnd even test the prediction from the user interface.\n\n![test](images/test-api.png)\n\nA next step is to infuse this model into the scoring application...\n\n## Notebook\n\nIn this section, we are developing a notebook on the telemetries dataset. We have already a project, and defined a data set from [the data collection step](../collect/cp4d-collect-data.md), so we need to add a notebook. For that, in the project view, use `Add to project` ...\n\n![add-to-project](images/add-to-project.png)\n\nand select the Notebook, \n\n![add-asset](images/add-asset.png)\n\nspecify a name and select python 3.6 runtime...\n\n![notebook](images/notebook-1.png)\n\n\nAdd a cell to access pandas, and maplablib in the first cell, and then get the code for accessing the dataset, by using the top right button:\n\n![first cells](images/notebook-2.png)\n\n\nTo Be Continued...\n","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/analyze/ws-ml-dev.mdx"}}}}