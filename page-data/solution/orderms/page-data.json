{"componentChunkName":"component---src-pages-solution-orderms-index-mdx","path":"/solution/orderms/","result":{"pageContext":{"frontmatter":{"title":"Vaccine Order Management","description":"This microservice manage order for vaccine"},"relativePagePath":"/solution/orderms/index.mdx","titleType":"append","MdxNode":{"id":"39e4d6d6-82c5-546a-9eee-48c68a82c870","children":[],"parent":"8d6b2b86-339a-5bf2-b9e2-7b84e98d8630","internal":{"content":"---\ntitle: Vaccine Order Management\ndescription: This microservice manage order for vaccine\n---\n<PageDescription>\nThis microservice manages vaccine orders for a world wide demand and distribution. This is an example of \n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build with s2i</AnchorLink>\n  <AnchorLink>Assess application runs</AnchorLink>\n  <AnchorLink>Demonstration script</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThis project implements a very simple event driven microservice to support the Create, Read, Update, of vaccine orders. This implementation highlights the following capbilities / patterns:\n\n* Quarkus reactive microservice using Microprofile 3.x - reactive messaging extension to consume Vaccine lot shipment plans\n* DB2 with Hibernate ORM with Quarkus Panache\n* Quarkus Debezium Outbox pattern to get update to the OrderEvents table using Debezium change data capture and DB2 change agent.\n\nIn term of business scenario, a sale representative uses his mobile device or web browser to enter information about a vaccine order to be shipped to a given country or a province within a country. \n\n ![](./images/order-ui.png)\n\nThe user interface is for demonstration purpose only and illustrates some standard Vuejs development practices. \n\n ![](./images/vaccine-order-1.png)\n\n\nThe component writes to the database (DB2) all the orders received, but also produces records to Kafka via the Outbox pattern and change data capture.\n\n**Github repository:** [Vaccine-order-mgr](https://github.com/ibm-cloud-architecture/vaccine-order-mgr)\n\n**Order Life Cycle**\n\nThe order follows a set of states as described in the following diagram:\n\n ![](./images/vaccine-order-2.png)\n\n**Kafka topics produced to:**\n\n\n**Events produced:**\n\nWe will simplify the process and aggregate in the following event types:\n\n* orderCreated\n* orderUpdated\n\nSome events are related to the vaccine lot\n\n* lotAssignedToOrder\n* lotLoaded\n* lotDelivered\n\n## Build with s2i\n\nThis microservice is built using maven and Quarkus extensions. We have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\nTo build and run locally see the [repository main readme](https://github.com/ibm-cloud-architecture/vaccine-order-mgr) as we have different docker-compose files to run in demonstration mode or in development mode.\n\nIn this section we address how to use OpenShift Source to Image to build and deploy the application to OpenShift. The application is using environment variables to access to user, password, URLs to access DB2, and Kafka deployed in Cloud Pak for Integration. So first thing is to define such secrets and config map.\n\nAs an example we created the OpenShift project called \"vaccine\" with a command: `oc new-project vaccine`.\n\n### Pre requisites\n\nThe orders are persisted in an external DB2 instance running on IBM Cloud Pack for Data. You need to get the username and password to connect to the DB2 instance with the jdbc URL (something like jdbc:db2://dashdb-....:50001/BLUDB)\n\nGet an instance of Kafka deployed on OpenShift like [IBM Event Streams](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#install-event-streams-using-operators)...). \n\n\n### Defining DB access secrets\n\nWe need to get IBM DB2 user credentials and JDBC URL as secrets. So from the DB2 service credentials get the following attributes: user, password, and JDBC URL. You need to encrypt each of those values using base64 encoding:\n\n```shell\n# Example of URL encryption\necho \"jdbc:db2://dashdb-txn-sbox-....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\" | base64\n```\n\nThen define a secret manifest with a command like:\n\n```shell\noc apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vaccine-order-db-secret\ndata:\n    DB_URL: ...encrypted URL...\n    DB_USER: ...encrypted user..\n    DB_PWD: ...encrypted pwd\nEOF\n```\n\nSee [the file here](https://github.com/ibm-cloud-architecture/vaccine-order-mgr/blob/master/src/main/kubernetes/secrets.yaml)\n\n### Get Event Streams credentials\n\nFor Event Streams get URL of the bootstrap server and the service credentials (TLS certificates) following [those instructions.](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift)\n\nWe assume Event Streams is deployed in `eventstreams` project and the solution is running under `vaccine` namespace.\n\n* Client application will use the server side public TLS certificate to connect to Kafka Brokers. See [those instructions](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#get-tls-server-public-certificate) to get the Server TLS certificate into the `certs` folder for local run as a secret for deploying to OpenShift. The following command copy the event streams ca cert secret to the current OpenShift project so it can be mounted to a path for the order microservice to get access to:\n\n ```shell\n # Under the 'vaccine' project\n oc get secret light-es-cluster-ca-cert   -n eventstreams --export -o yaml | oc apply -f -\n ```\n\n* Modify the config map from [src/main/kubernetes/configmap.yaml](https://github.com/ibm-cloud-architecture/vaccine-order-mgr/blob/master/src/main/kubernetes/configmap.yaml) with the URL of your bootstrap server.\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vaccine-order-ms-cm\ndata:\n  KAFKA_BOOTSTRAP_SERVERS: \"light-es-kafka-bootstrap.eventstreams.svc:9092\"\n  KAFKA_SSL_PROTOCOL: \"TLSv1.2\"\n  KAFKA_SSL_TRUSTSTORE_LOCATION: \"/deployments/certs/server/ca.p12\"\n  KAFKA_SSL_TRUSTSTORE_TYPE: \"PKCS12\"\n  SHIPMENT_PLAN_TOPIC: \"vaccine_shipment_plans\"\n```\n\nThose environment variables are used by the application, and configured via the [src/main/resources/application.properties](https://github.com/ibm-cloud-architecture/vaccine-order-mgr/blob/master/src/main/resources/application.properties). Below is the declarations that define environment variables from the previously created secrets and configmap.\n\n```propertiies\nquarkus.openshift.env.configmaps=vaccine-order-ms-cm\nquarkus.openshift.env.secrets=vaccine-order-secrets\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=light-es-cluster-ca-cert\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=light-es-cluster-ca-cert\n```\n\n### Remote access to event streams\n\n* If you need to access Event Streams from an application running outside of CP4I cluster then create a scram access user. Verify the user is created with `oc get kafkausers -n eventstreams`, and then add the following declarations into the config maps:\n\n  ```properties\n  KAFKA_SECURITY_PROTOCOL: \"SASL_SSL\"\n  KAFKA_SASL_MECHANISM: \"SCRAM-SHA-512\"\n  KAFKA_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PWD}\\\";\"\n  ```\n\n\n### Build and deploy\n\n```shell\n# if not logged yes to your openshift cluster where the docker private registry resides do:\noc login --token=... --server=https://c...\n\n# Then build the code using source 2 image and push the image to the internal registry\nmvn package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests\n```\n\nIt can take some seconds to build and deploy: `oc get pods -w` lets you see the build pods and the running app once the build is done. As we set properties to expose the application, an OpenShift route was created. The url is visible at the end of the build output, something like:\n\n...The deployed application can be accessed at: http://quarkus-kstreams-lab3...\n\n## Demonstration script\n\n### User interface \n\n\n### REST APIs\n\nThe REST end point for this service expose the following OpenAPI:\n\n ![4](./images/openapi.png)\n\n","type":"Mdx","contentDigest":"53991467942c7b560f0b9806905edd0b","counter":247,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Vaccine Order Management","description":"This microservice manage order for vaccine"},"exports":{},"rawBody":"---\ntitle: Vaccine Order Management\ndescription: This microservice manage order for vaccine\n---\n<PageDescription>\nThis microservice manages vaccine orders for a world wide demand and distribution. This is an example of \n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Build with s2i</AnchorLink>\n  <AnchorLink>Assess application runs</AnchorLink>\n  <AnchorLink>Demonstration script</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThis project implements a very simple event driven microservice to support the Create, Read, Update, of vaccine orders. This implementation highlights the following capbilities / patterns:\n\n* Quarkus reactive microservice using Microprofile 3.x - reactive messaging extension to consume Vaccine lot shipment plans\n* DB2 with Hibernate ORM with Quarkus Panache\n* Quarkus Debezium Outbox pattern to get update to the OrderEvents table using Debezium change data capture and DB2 change agent.\n\nIn term of business scenario, a sale representative uses his mobile device or web browser to enter information about a vaccine order to be shipped to a given country or a province within a country. \n\n ![](./images/order-ui.png)\n\nThe user interface is for demonstration purpose only and illustrates some standard Vuejs development practices. \n\n ![](./images/vaccine-order-1.png)\n\n\nThe component writes to the database (DB2) all the orders received, but also produces records to Kafka via the Outbox pattern and change data capture.\n\n**Github repository:** [Vaccine-order-mgr](https://github.com/ibm-cloud-architecture/vaccine-order-mgr)\n\n**Order Life Cycle**\n\nThe order follows a set of states as described in the following diagram:\n\n ![](./images/vaccine-order-2.png)\n\n**Kafka topics produced to:**\n\n\n**Events produced:**\n\nWe will simplify the process and aggregate in the following event types:\n\n* orderCreated\n* orderUpdated\n\nSome events are related to the vaccine lot\n\n* lotAssignedToOrder\n* lotLoaded\n* lotDelivered\n\n## Build with s2i\n\nThis microservice is built using maven and Quarkus extensions. We have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\nTo build and run locally see the [repository main readme](https://github.com/ibm-cloud-architecture/vaccine-order-mgr) as we have different docker-compose files to run in demonstration mode or in development mode.\n\nIn this section we address how to use OpenShift Source to Image to build and deploy the application to OpenShift. The application is using environment variables to access to user, password, URLs to access DB2, and Kafka deployed in Cloud Pak for Integration. So first thing is to define such secrets and config map.\n\nAs an example we created the OpenShift project called \"vaccine\" with a command: `oc new-project vaccine`.\n\n### Pre requisites\n\nThe orders are persisted in an external DB2 instance running on IBM Cloud Pack for Data. You need to get the username and password to connect to the DB2 instance with the jdbc URL (something like jdbc:db2://dashdb-....:50001/BLUDB)\n\nGet an instance of Kafka deployed on OpenShift like [IBM Event Streams](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#install-event-streams-using-operators)...). \n\n\n### Defining DB access secrets\n\nWe need to get IBM DB2 user credentials and JDBC URL as secrets. So from the DB2 service credentials get the following attributes: user, password, and JDBC URL. You need to encrypt each of those values using base64 encoding:\n\n```shell\n# Example of URL encryption\necho \"jdbc:db2://dashdb-txn-sbox-....services.dal.bluemix.net:50001/BLUDB:sslConnection=true;\" | base64\n```\n\nThen define a secret manifest with a command like:\n\n```shell\noc apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vaccine-order-db-secret\ndata:\n    DB_URL: ...encrypted URL...\n    DB_USER: ...encrypted user..\n    DB_PWD: ...encrypted pwd\nEOF\n```\n\nSee [the file here](https://github.com/ibm-cloud-architecture/vaccine-order-mgr/blob/master/src/main/kubernetes/secrets.yaml)\n\n### Get Event Streams credentials\n\nFor Event Streams get URL of the bootstrap server and the service credentials (TLS certificates) following [those instructions.](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#getting-tls-authentication-from-event-streams-on-openshift)\n\nWe assume Event Streams is deployed in `eventstreams` project and the solution is running under `vaccine` namespace.\n\n* Client application will use the server side public TLS certificate to connect to Kafka Brokers. See [those instructions](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/overview/pre-requisites#get-tls-server-public-certificate) to get the Server TLS certificate into the `certs` folder for local run as a secret for deploying to OpenShift. The following command copy the event streams ca cert secret to the current OpenShift project so it can be mounted to a path for the order microservice to get access to:\n\n ```shell\n # Under the 'vaccine' project\n oc get secret light-es-cluster-ca-cert   -n eventstreams --export -o yaml | oc apply -f -\n ```\n\n* Modify the config map from [src/main/kubernetes/configmap.yaml](https://github.com/ibm-cloud-architecture/vaccine-order-mgr/blob/master/src/main/kubernetes/configmap.yaml) with the URL of your bootstrap server.\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vaccine-order-ms-cm\ndata:\n  KAFKA_BOOTSTRAP_SERVERS: \"light-es-kafka-bootstrap.eventstreams.svc:9092\"\n  KAFKA_SSL_PROTOCOL: \"TLSv1.2\"\n  KAFKA_SSL_TRUSTSTORE_LOCATION: \"/deployments/certs/server/ca.p12\"\n  KAFKA_SSL_TRUSTSTORE_TYPE: \"PKCS12\"\n  SHIPMENT_PLAN_TOPIC: \"vaccine_shipment_plans\"\n```\n\nThose environment variables are used by the application, and configured via the [src/main/resources/application.properties](https://github.com/ibm-cloud-architecture/vaccine-order-mgr/blob/master/src/main/resources/application.properties). Below is the declarations that define environment variables from the previously created secrets and configmap.\n\n```propertiies\nquarkus.openshift.env.configmaps=vaccine-order-ms-cm\nquarkus.openshift.env.secrets=vaccine-order-secrets\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=light-es-cluster-ca-cert\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=light-es-cluster-ca-cert\n```\n\n### Remote access to event streams\n\n* If you need to access Event Streams from an application running outside of CP4I cluster then create a scram access user. Verify the user is created with `oc get kafkausers -n eventstreams`, and then add the following declarations into the config maps:\n\n  ```properties\n  KAFKA_SECURITY_PROTOCOL: \"SASL_SSL\"\n  KAFKA_SASL_MECHANISM: \"SCRAM-SHA-512\"\n  KAFKA_SASL_JAAS_CONFIG: \"org.apache.kafka.common.security.scram.ScramLoginModule required username\\=\\\"${KAFKA_USER}\\\" password\\=\\\"${KAFKA_PWD}\\\";\"\n  ```\n\n\n### Build and deploy\n\n```shell\n# if not logged yes to your openshift cluster where the docker private registry resides do:\noc login --token=... --server=https://c...\n\n# Then build the code using source 2 image and push the image to the internal registry\nmvn package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests\n```\n\nIt can take some seconds to build and deploy: `oc get pods -w` lets you see the build pods and the running app once the build is done. As we set properties to expose the application, an OpenShift route was created. The url is visible at the end of the build output, something like:\n\n...The deployed application can be accessed at: http://quarkus-kstreams-lab3...\n\n## Demonstration script\n\n### User interface \n\n\n### REST APIs\n\nThe REST end point for this service expose the following OpenAPI:\n\n ![4](./images/openapi.png)\n\n","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/solution/orderms/index.mdx"}}}}