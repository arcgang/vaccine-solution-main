{"componentChunkName":"component---src-pages-use-cases-order-index-mdx","path":"/use-cases/order/","result":{"pageContext":{"frontmatter":{"title":"Order management and optimization demo","description":"Order management and optimization demonstration"},"relativePagePath":"/use-cases/order/index.mdx","titleType":"append","MdxNode":{"id":"28a2ff34-3b90-5a3a-9e6b-1bf494dc9baf","children":[],"parent":"544dd348-9957-5f89-85d0-7ce6e293e2fe","internal":{"content":"---\ntitle: Order management and optimization demo\ndescription: Order management and optimization demonstration\n--- \n\nThe vaccine order fullfilment is a complex problem to address as we need to optimize the shipment plan according to the demand and the contraints. A deep dive to the O/R problem is described in [this note](/solution/orderms/voro-solution/). In this scenario we want to demostrate the order entry with an event-driven microservice, and the integration with an event driven linear programming stateful function. The scenario addresses the following features:\n\n* Create Order(s) using a simple user interface as an order manager will do after interacting with the country health care request.\n* Validate the [transactional outbox pattern](https://ibm-cloud-architecture.github.io/refarch-eda/patterns/intro/#transactional-outbox) works to get OrderCreated, OrderUpdated or OrderCancelled Events created into a dedicated table in Postgresql\n* Validate how [Debezium Change Data Capture](https://debezium.io/documentation/reference/1.4/connectors/postgresql.html) for Postgresql as a Kafka connector, produces OrderEvents from the table to Kafka `orders` topic.\n* Integrate with the [Shipment plan optimization]() to get the updated shipment plan as events.\n\n## Components involved in this use case\n\n* [Vaccine Order Service](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg)\n* Postgres DB\n* Debezium for change data capture on the outbox table\n* Kafka or [event streams](https://ibm.github.io/event-streams/)\n* [Vaccine Order Reefer Optimization Service](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer)\n\n ![](../../solution/orderms/images/vaccine-order-1.png)\n\n*The blockchain integration is not done yet*\n\nTo have a deeper understanding of the order service read [this overview section](/solution/orderms/#overview). For the Vaccine order reefer optimization service [the design article](/design/voro/) details the optimization approach, while the [solution article](/solution/orderms/voro-solution/#overview) goes over the service implementation details.\n\n## Run to OpenShift \n\n### Pre-requisites\n\n* You need:\n    * docker and docker compose on your laptop to run locally or build image\n    * Java 11\n    * git client \n    * Get access to an OpenShift cluster with Cloud Pak for integration and event streams installed.\n1. Create the following artifacts in the `eventstreams` namespace on your OpenShift cluster:\n\n   1. Create an EventStreams instance _(via the [Event Streams Custom Resource](https://ibm.github.io/event-streams/installing/installing/#install-an-event-streams-instance))_.\n   2. Create a [Kafka User with SCRAM-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Reefer Simulator](https://github.com/ibm-cloud-architecture/vaccine-reefer-simulator#application-deployment).\n   3. Create a [Kafka User with TLS-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Monitoring Agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent#create-a-tls-user).\n   4. Create two [Kafka Topics](https://ibm.github.io/event-streams/getting-started/creating-topics/). This tutorial will assume the names of `vaccine.shipment.plans` respectively.\n\n1. Create a new project named `vaccineorder` that will be used for the deployment of all other components.\n1. [OpenShift CLI](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli) on your local environment.\n1. [jq](https://stedolan.github.io/jq/) on your local environment.\n1. Use a Terminal and the oc cli. If you want to access the code source you can clone the two main repositories of this solution:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg\n git clone https://github.com/ibm-cloud-architecture/vaccine-order-optimization-pg\n ```\n\n### Deploy the Vaccine Order Service\n\nThis microservice is built using maven and Quarkus extensions. In this current main project we have \nWe have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\n1. Ensure you are working inside the correct project via the following `oc` command:\n\n  ```shell\n  export PROJECT_NAME=vaccineorder\n  oc project ${PROJECT_NAME}\n  ```\n\n1. Export the value of your Event Streams cluster name into an environment variable:\n\n  ```shell\n  export CLUSTER_NAME=eda-dev\n  export EVENTSTREAMS_NS=eventstreams\n  ```\n   * To check what the name of your Event Streams cluster name is do:\n   ```shell\n   $ oc get eventstreams -n ${EVENTSTREAMS_NS}\n   NAME           STATUS\n   development    Ready\n   ```\n1. Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:\n\n  ```shell\n  # Define environement variables\n  SERVICE_ACCOUNT_NAME=postgres-sa\n  DEPLOYMENT_NAME=postgres\n  SERVICE_NAME=postgres\n  DOCKER_IMAGE=docker.io/postgres:11.6-alpine\n  POSTGRES_PASSWORD=adifficultpasswordtoguess\n\n  oc create serviceaccount ${SERVICE_ACCOUNT_NAME}\n  oc adm policy add-scc-to-user anyuid -n ${PROJECT_NAME} -z ${SERVICE_ACCOUNT_NAME}\n  oc create deployment ${DEPLOYMENT_NAME} --image=${DOCKER_IMAGE}\n  oc set serviceaccount deployment/${DEPLOYMENT_NAME} ${SERVICE_ACCOUNT_NAME}\n  oc patch deployment ${DEPLOYMENT_NAME} --type=\"json\" -p='[{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args\", \"value\":[]},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"-c\"},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"wal_level=logical\"} ]'\n  oc set env deployment ${DEPLOYMENT_NAME} POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n  oc expose deployment ${DEPLOYMENT_NAME} --port 5432 --name ${SERVICE_NAME}\n  ```\n\n1. Get Kafka TLS user name to access Event Streams bootstrap using the external route. export this user name in KAFKA_USER environment variable:\n\n  ```shell\n  oc get kafkausers -n $EVENTSTREAMS_NS \n  # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION\n  # app-scram                           eda-dev   scram-sha-512    simple\n  # app-tls                             eda-dev   tls              simple\n  export KAFKA_USER=app-tls\n  ```\n1. Copy Kafka TLS user certificate to target project:\n\n  ```shell\n  oc get secret ${KAFKA_USER} -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n  ```\n1. Get Kafka bootstrap server URL within\n\n   ```shell\n   export KAFKA_BROKERS=$(oc get route -n ${EVENTSTREAMS_NS} ${CLUSTER_NAME}-kafka-bootstrap -o jsonpath=\"{.status.ingress[0].host}:443\")\n   ```\n\n 1. Copy TLS server CA certificate from eventstreams project to local project with the command:\n\n   ```shell\n   oc get secret ${CLUSTER_NAME}-cluster-ca-cert -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.name=\"kafka-cluster-ca-cert\"' |jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n   ``` \n\n1. Define environment variables for the order service with config map and secrets:\n\n```shell\n# config map\noc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/configmap.yaml\n# Secrets\noc apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vaccine-order-secrets\nstringData:\n  KAFKA_USER: ${KAFKA_USER}\n  KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_USER}\n  QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}\n  QUARKUS_DATASOURCE_USERNAME: postgres\n```\n\n* Deploy the app. If you have cloned the repository and have java you can use the following command:\n\n ```shell\n ./mvnw clean package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests\n ```\n\n If not, you can deploy the application using the deployment config as we have pushed the ibmcase/vaccineorders image to dockerhub:\n\n ```shell\noc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/deploymentconfig.yaml\n\n ```\n\n### Deploy Debezium CDC connector\n\nThe Event Streams product documentation goes over the tasks to be done, but to summarize them, here are the actions done to deploy the postgres debezium connector:\n\n* Start a Kafka connector cluster: We use the custom resource called `KafkaConnectS2I`, which one instance represents a Kafka connect cluster. Each connector is represented by another custom resource called KafkaConnector. Kafka connect needs a user access to the Brokers. We can use the TLS user as previously done. \n  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.\n  * Modify the bootstrap server and the TLS user to be used. Ensure the user can access any topics.\n  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n eventstreams`\n  * Validate it via: \n\n   ```shell\n   oc get kafkaconnects2i -n eventstreams\n   oc describe kafkaconnects2i connect-cluster -n eventstreams\n   ```\n* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. \n\n  ```\n  ├── my-plugins\n  │   └── debezium-connector\n  │       ├── CHANGELOG.md\n  │       ├── CONTRIBUTE.md\n  │       ├── COPYRIGHT.txt\n  │       ├── LICENSE-3rd-PARTIES.txt\n  │       ├── LICENSE.txt\n  │       ├── README.md\n  │       ├── README_ZH.md\n  │       ├── debezium-api-1.4.0.Final.jar\n  │       ├── debezium-connector-postgres-1.4.0.Final.jar\n  │       ├── debezium-core-1.4.0.Final.jar\n  │       ├── failureaccess-1.0.1.jar\n  │       ├── guava-30.0-jre.jar\n  │       ├── postgresql-42.2.14.jar\n  │       └── protobuf-java-3.8.0.jar\n  └── pg-connector.yaml\n  ```\n\n* Deploy the connector configuration:\n\n  ```shell\n  oc start-build connect-cluster-connect --from-dir ./my-plugins/\n  oc get builds\n  # NAME                        TYPE     FROM             STATUS    STARTED          DURATION\n  # connect-cluster-connect-3   Source   Binary@f639186   Running   19 seconds ago   \n  # once build is completed... wait to get the 3 kafka connect workers ready\n  oc get pods -w\n  ```\n* Start the connector: `oc apply -f pg-connector.yaml`\n* Verify it is running: `oc describe kafkaconnector pg-connector` \n\n### Deploy the Optimization Service\n\n## Scenario script\n\nOnce the solution is up and running in your target deployment environment (local or OpenShift) execute the following steps to present an end to end demonstration.\n\n\n### Validate existing vaccine lot shipment plan\n\n### Create an order from the user interface of the Order Service. \n\n### See how the shipment plan is modified\n\n### Validate the blockchain records\n\n### Present the events created\n\n","type":"Mdx","contentDigest":"31a1ff02f7f0be04894a6f1f35db2f75","counter":325,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Order management and optimization demo","description":"Order management and optimization demonstration"},"exports":{},"rawBody":"---\ntitle: Order management and optimization demo\ndescription: Order management and optimization demonstration\n--- \n\nThe vaccine order fullfilment is a complex problem to address as we need to optimize the shipment plan according to the demand and the contraints. A deep dive to the O/R problem is described in [this note](/solution/orderms/voro-solution/). In this scenario we want to demostrate the order entry with an event-driven microservice, and the integration with an event driven linear programming stateful function. The scenario addresses the following features:\n\n* Create Order(s) using a simple user interface as an order manager will do after interacting with the country health care request.\n* Validate the [transactional outbox pattern](https://ibm-cloud-architecture.github.io/refarch-eda/patterns/intro/#transactional-outbox) works to get OrderCreated, OrderUpdated or OrderCancelled Events created into a dedicated table in Postgresql\n* Validate how [Debezium Change Data Capture](https://debezium.io/documentation/reference/1.4/connectors/postgresql.html) for Postgresql as a Kafka connector, produces OrderEvents from the table to Kafka `orders` topic.\n* Integrate with the [Shipment plan optimization]() to get the updated shipment plan as events.\n\n## Components involved in this use case\n\n* [Vaccine Order Service](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg)\n* Postgres DB\n* Debezium for change data capture on the outbox table\n* Kafka or [event streams](https://ibm.github.io/event-streams/)\n* [Vaccine Order Reefer Optimization Service](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer)\n\n ![](../../solution/orderms/images/vaccine-order-1.png)\n\n*The blockchain integration is not done yet*\n\nTo have a deeper understanding of the order service read [this overview section](/solution/orderms/#overview). For the Vaccine order reefer optimization service [the design article](/design/voro/) details the optimization approach, while the [solution article](/solution/orderms/voro-solution/#overview) goes over the service implementation details.\n\n## Run to OpenShift \n\n### Pre-requisites\n\n* You need:\n    * docker and docker compose on your laptop to run locally or build image\n    * Java 11\n    * git client \n    * Get access to an OpenShift cluster with Cloud Pak for integration and event streams installed.\n1. Create the following artifacts in the `eventstreams` namespace on your OpenShift cluster:\n\n   1. Create an EventStreams instance _(via the [Event Streams Custom Resource](https://ibm.github.io/event-streams/installing/installing/#install-an-event-streams-instance))_.\n   2. Create a [Kafka User with SCRAM-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Reefer Simulator](https://github.com/ibm-cloud-architecture/vaccine-reefer-simulator#application-deployment).\n   3. Create a [Kafka User with TLS-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Monitoring Agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent#create-a-tls-user).\n   4. Create two [Kafka Topics](https://ibm.github.io/event-streams/getting-started/creating-topics/). This tutorial will assume the names of `vaccine.shipment.plans` respectively.\n\n1. Create a new project named `vaccineorder` that will be used for the deployment of all other components.\n1. [OpenShift CLI](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli) on your local environment.\n1. [jq](https://stedolan.github.io/jq/) on your local environment.\n1. Use a Terminal and the oc cli. If you want to access the code source you can clone the two main repositories of this solution:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg\n git clone https://github.com/ibm-cloud-architecture/vaccine-order-optimization-pg\n ```\n\n### Deploy the Vaccine Order Service\n\nThis microservice is built using maven and Quarkus extensions. In this current main project we have \nWe have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\n1. Ensure you are working inside the correct project via the following `oc` command:\n\n  ```shell\n  export PROJECT_NAME=vaccineorder\n  oc project ${PROJECT_NAME}\n  ```\n\n1. Export the value of your Event Streams cluster name into an environment variable:\n\n  ```shell\n  export CLUSTER_NAME=eda-dev\n  export EVENTSTREAMS_NS=eventstreams\n  ```\n   * To check what the name of your Event Streams cluster name is do:\n   ```shell\n   $ oc get eventstreams -n ${EVENTSTREAMS_NS}\n   NAME           STATUS\n   development    Ready\n   ```\n1. Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:\n\n  ```shell\n  # Define environement variables\n  SERVICE_ACCOUNT_NAME=postgres-sa\n  DEPLOYMENT_NAME=postgres\n  SERVICE_NAME=postgres\n  DOCKER_IMAGE=docker.io/postgres:11.6-alpine\n  POSTGRES_PASSWORD=adifficultpasswordtoguess\n\n  oc create serviceaccount ${SERVICE_ACCOUNT_NAME}\n  oc adm policy add-scc-to-user anyuid -n ${PROJECT_NAME} -z ${SERVICE_ACCOUNT_NAME}\n  oc create deployment ${DEPLOYMENT_NAME} --image=${DOCKER_IMAGE}\n  oc set serviceaccount deployment/${DEPLOYMENT_NAME} ${SERVICE_ACCOUNT_NAME}\n  oc patch deployment ${DEPLOYMENT_NAME} --type=\"json\" -p='[{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args\", \"value\":[]},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"-c\"},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"wal_level=logical\"} ]'\n  oc set env deployment ${DEPLOYMENT_NAME} POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n  oc expose deployment ${DEPLOYMENT_NAME} --port 5432 --name ${SERVICE_NAME}\n  ```\n\n1. Get Kafka TLS user name to access Event Streams bootstrap using the external route. export this user name in KAFKA_USER environment variable:\n\n  ```shell\n  oc get kafkausers -n $EVENTSTREAMS_NS \n  # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION\n  # app-scram                           eda-dev   scram-sha-512    simple\n  # app-tls                             eda-dev   tls              simple\n  export KAFKA_USER=app-tls\n  ```\n1. Copy Kafka TLS user certificate to target project:\n\n  ```shell\n  oc get secret ${KAFKA_USER} -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n  ```\n1. Get Kafka bootstrap server URL within\n\n   ```shell\n   export KAFKA_BROKERS=$(oc get route -n ${EVENTSTREAMS_NS} ${CLUSTER_NAME}-kafka-bootstrap -o jsonpath=\"{.status.ingress[0].host}:443\")\n   ```\n\n 1. Copy TLS server CA certificate from eventstreams project to local project with the command:\n\n   ```shell\n   oc get secret ${CLUSTER_NAME}-cluster-ca-cert -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.name=\"kafka-cluster-ca-cert\"' |jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n   ``` \n\n1. Define environment variables for the order service with config map and secrets:\n\n```shell\n# config map\noc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/configmap.yaml\n# Secrets\noc apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vaccine-order-secrets\nstringData:\n  KAFKA_USER: ${KAFKA_USER}\n  KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_USER}\n  QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}\n  QUARKUS_DATASOURCE_USERNAME: postgres\n```\n\n* Deploy the app. If you have cloned the repository and have java you can use the following command:\n\n ```shell\n ./mvnw clean package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests\n ```\n\n If not, you can deploy the application using the deployment config as we have pushed the ibmcase/vaccineorders image to dockerhub:\n\n ```shell\noc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/deploymentconfig.yaml\n\n ```\n\n### Deploy Debezium CDC connector\n\nThe Event Streams product documentation goes over the tasks to be done, but to summarize them, here are the actions done to deploy the postgres debezium connector:\n\n* Start a Kafka connector cluster: We use the custom resource called `KafkaConnectS2I`, which one instance represents a Kafka connect cluster. Each connector is represented by another custom resource called KafkaConnector. Kafka connect needs a user access to the Brokers. We can use the TLS user as previously done. \n  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.\n  * Modify the bootstrap server and the TLS user to be used. Ensure the user can access any topics.\n  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n eventstreams`\n  * Validate it via: \n\n   ```shell\n   oc get kafkaconnects2i -n eventstreams\n   oc describe kafkaconnects2i connect-cluster -n eventstreams\n   ```\n* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. \n\n  ```\n  ├── my-plugins\n  │   └── debezium-connector\n  │       ├── CHANGELOG.md\n  │       ├── CONTRIBUTE.md\n  │       ├── COPYRIGHT.txt\n  │       ├── LICENSE-3rd-PARTIES.txt\n  │       ├── LICENSE.txt\n  │       ├── README.md\n  │       ├── README_ZH.md\n  │       ├── debezium-api-1.4.0.Final.jar\n  │       ├── debezium-connector-postgres-1.4.0.Final.jar\n  │       ├── debezium-core-1.4.0.Final.jar\n  │       ├── failureaccess-1.0.1.jar\n  │       ├── guava-30.0-jre.jar\n  │       ├── postgresql-42.2.14.jar\n  │       └── protobuf-java-3.8.0.jar\n  └── pg-connector.yaml\n  ```\n\n* Deploy the connector configuration:\n\n  ```shell\n  oc start-build connect-cluster-connect --from-dir ./my-plugins/\n  oc get builds\n  # NAME                        TYPE     FROM             STATUS    STARTED          DURATION\n  # connect-cluster-connect-3   Source   Binary@f639186   Running   19 seconds ago   \n  # once build is completed... wait to get the 3 kafka connect workers ready\n  oc get pods -w\n  ```\n* Start the connector: `oc apply -f pg-connector.yaml`\n* Verify it is running: `oc describe kafkaconnector pg-connector` \n\n### Deploy the Optimization Service\n\n## Scenario script\n\nOnce the solution is up and running in your target deployment environment (local or OpenShift) execute the following steps to present an end to end demonstration.\n\n\n### Validate existing vaccine lot shipment plan\n\n### Create an order from the user interface of the Order Service. \n\n### See how the shipment plan is modified\n\n### Validate the blockchain records\n\n### Present the events created\n\n","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/use-cases/order/index.mdx"}}}}