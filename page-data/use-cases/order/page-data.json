{"componentChunkName":"component---src-pages-use-cases-order-index-mdx","path":"/use-cases/order/","result":{"pageContext":{"frontmatter":{"title":"Order management and optimization demo","description":"Order management and optimization demonstration"},"relativePagePath":"/use-cases/order/index.mdx","titleType":"append","MdxNode":{"id":"28a2ff34-3b90-5a3a-9e6b-1bf494dc9baf","children":[],"parent":"544dd348-9957-5f89-85d0-7ce6e293e2fe","internal":{"content":"---\ntitle: Order management and optimization demo\ndescription: Order management and optimization demonstration\n--- \n\nThe vaccine order fullfilment is a complex problem to address as we need to optimize the shipment plan according to the demand and the contraints. A deep dive to the O/R problem is described in [this note](/solution/orderms/voro-solution/). In this scenario we want to demostrate the order entry with an event-driven microservice, and the integration with an event driven linear programming stateful function. The scenario addresses the following features:\n\n* Create Order(s) using a simple user interface as an order manager will do after interacting with the country health care request.\n* Validate the [transactional outbox pattern](https://ibm-cloud-architecture.github.io/refarch-eda/patterns/intro/#transactional-outbox) works to get OrderCreated, OrderUpdated or OrderCancelled Events created into a dedicated table in Postgresql\n* Validate how [Debezium Change Data Capture](https://debezium.io/documentation/reference/1.4/connectors/postgresql.html) for Postgresql as a Kafka connector, produces OrderEvents from the table to Kafka `orders` topic.\n* Integrate with the [Shipment plan optimization]() to get the updated shipment plan as events.\n\n## Components involved in this use case\n\n* [Vaccine Order Service](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg)\n* Postgres DB\n* Debezium for change data capture on the outbox table\n* Kafka or [event streams](https://ibm.github.io/event-streams/)\n* [Vaccine Order Reefer Optimization Service](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer)\n\n ![](../../solution/orderms/images/vaccine-order-1.png)\n\n*The blockchain integration is not done yet*\n\nTo have a deeper understanding of the order service read [this overview section](/solution/orderms/#overview). For the Vaccine order reefer optimization service [the design article](/design/voro/) details the optimization approach, while the [solution article](/solution/orderms/voro-solution/#overview) goes over the service implementation details.\n\n## Run to OpenShift \n\n### Pre-requisites\n\n* You need:\n    * docker and docker compose on your laptop to run locally or build image\n    * Java 11\n    * git client \n    * Get access to an OpenShift cluster with Cloud Pak for integration and event streams installed.\n1. Create the following artifacts in the `eventstreams` namespace on your OpenShift cluster:\n\n   1. Create an EventStreams instance _(via the [Event Streams Custom Resource](https://ibm.github.io/event-streams/installing/installing/#install-an-event-streams-instance))_.\n   2. Create a [Kafka User with SCRAM-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Reefer Simulator](https://github.com/ibm-cloud-architecture/vaccine-reefer-simulator#application-deployment).\n   3. Create a [Kafka User with TLS-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Monitoring Agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent#create-a-tls-user).\n   4. Create two [Kafka Topics](https://ibm.github.io/event-streams/getting-started/creating-topics/). This tutorial will assume the names of `vaccine.shipment.plans` respectively.\n\n1. Create a new project named `vaccineorder` that will be used for the deployment of all other components.\n1. [OpenShift CLI](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli) on your local environment.\n1. [jq](https://stedolan.github.io/jq/) on your local environment.\n1. Use a Terminal and the oc cli. If you want to access the code source you can clone the two main repositories of this solution:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg\n git clone https://github.com/ibm-cloud-architecture/vaccine-order-optimization-pg\n ```\n\n### Deploy the Vaccine Order Service\n\nThis microservice is built using maven and Quarkus extensions. In this current main project we have \nWe have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\n1. Ensure you are working inside the correct project via the following `oc` command:\n\n  ```shell\n  export PROJECT_NAME=vaccineorder\n  oc project ${PROJECT_NAME}\n  ```\n\n1. Export the value of your Event Streams cluster name into an environment variable:\n\n  ```shell\n  export CLUSTER_NAME=eda-dev\n  export EVENTSTREAMS_NS=eventstreams\n  ```\n   * To check what the name of your Event Streams cluster name is do:\n   ```shell\n   $ oc get eventstreams -n ${EVENTSTREAMS_NS}\n   NAME           STATUS\n   development    Ready\n   ```\n1. Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:\n\n  ```shell\n  # Define environement variables\n  SERVICE_ACCOUNT_NAME=postgres-sa\n  DEPLOYMENT_NAME=postgres\n  SERVICE_NAME=postgres\n  DOCKER_IMAGE=docker.io/postgres:11.6-alpine\n  POSTGRES_PASSWORD=adifficultpasswordtoguess\n\n  oc create serviceaccount ${SERVICE_ACCOUNT_NAME}\n  oc adm policy add-scc-to-user anyuid -n ${PROJECT_NAME} -z ${SERVICE_ACCOUNT_NAME}\n  oc create deployment ${DEPLOYMENT_NAME} --image=${DOCKER_IMAGE}\n  oc set serviceaccount deployment/${DEPLOYMENT_NAME} ${SERVICE_ACCOUNT_NAME}\n  oc patch deployment ${DEPLOYMENT_NAME} --type=\"json\" -p='[{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args\", \"value\":[]},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"-c\"},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"wal_level=logical\"} ]'\n  oc set env deployment ${DEPLOYMENT_NAME} POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n  oc expose deployment ${DEPLOYMENT_NAME} --port 5432 --name ${SERVICE_NAME}\n  ```\n\n1. Get Kafka TLS user name to access Event Streams bootstrap using the external route. export this user name in KAFKA_USER environment variable:\n\n  ```shell\n  oc get kafkausers -n $EVENTSTREAMS_NS \n  # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION\n  # app-scram                           eda-dev   scram-sha-512    simple\n  # app-tls                             eda-dev   tls              simple\n  export KAFKA_USER=app-tls\n  ```\n1. Copy Kafka TLS user certificate to target project:\n\n  ```shell\n  oc get secret ${KAFKA_USER} -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n  ```\n1. Get Kafka bootstrap server URL within\n\n   ```shell\n   export KAFKA_BROKERS=$(oc get route -n ${EVENTSTREAMS_NS} ${CLUSTER_NAME}-kafka-bootstrap -o jsonpath=\"{.status.ingress[0].host}:443\")\n   ```\n\n 1. Copy TLS server CA certificate from eventstreams project to local project with the command:\n\n   ```shell\n   oc get secret ${CLUSTER_NAME}-cluster-ca-cert -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.name=\"kafka-cluster-ca-cert\"' |jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n   ``` \n\n1. Define environment variables for the order service with config map and secrets:\n\n```shell\n# config map\noc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/configmap.yaml\n# Secrets\noc apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vaccine-order-secrets\nstringData:\n  KAFKA_USER: ${KAFKA_USER}\n  KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_USER}\n  QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}\n  QUARKUS_DATASOURCE_USERNAME: postgres\n```\n\n* Deploy the app. If you have cloned the repository and have java you can use the following command:\n\n ```shell\n ./mvnw clean package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests\n ```\n\n If not, you can deploy the application using the deployment config as we have pushed the ibmcase/vaccineorders image to dockerhub:\n\n ```shell\noc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/deploymentconfig.yaml\n\n ```\n\n### Deploy Debezium CDC connector\n\nThe [Event Streams product documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) goes over the tasks to be done to config Kafka Connect cluster, but we can summarize them for our use case as:\n\n* Start a Kafka connector cluster: We use the custom resource definition called `KafkaConnectS2I`, which one instance represents a Kafka connect cluster. Each connector is represented by another custom resource called `KafkaConnector`. Kafka connect needs a user access to the Brokers. We can use the TLS user as previously done (See also [this note](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/connect-cos/#set-up-the-kafka-connect-cluster) for the same type of setting). \n  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.\n  * Update the following values in this file: `bootstrapServers` and `secretName: tls`  to `secretName: <yourTLSuser>` and the Server ca certificate secretName like `kafka-cluster-ca-cert`.\n   ```yaml\n   tls:\n    trustedCertificates:\n      - certificate: ca.crt\n        secretName: kafka-cluster-ca-cert\n   ```\n  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n ${EVENTSTREAMS_NS}`\n  * Validate it via: \n\n   ```shell\n   oc get kafkaconnects2i -n ${EVENTSTREAMS_NS}\n   oc describe kafkaconnects2i connect-cluster -n ${EVENTSTREAMS_NS}\n   ```\n* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. This step was already done and the debezium connector jars are in the [environment/cdc/my-plugins/debezium-connector)](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/tree/main/environment/cdc/my-plugins/debezium-connector)\n\n  ```\n  ├── my-plugins\n  │   └── debezium-connector\n  │       ├── CHANGELOG.md\n  │       ├── CONTRIBUTE.md\n  │       ├── COPYRIGHT.txt\n  │       ├── LICENSE-3rd-PARTIES.txt\n  │       ├── LICENSE.txt\n  │       ├── README.md\n  │       ├── README_ZH.md\n  │       ├── debezium-api-1.4.0.Final.jar\n  │       ├── debezium-connector-postgres-1.4.0.Final.jar\n  │       ├── debezium-core-1.4.0.Final.jar\n  │       ├── failureaccess-1.0.1.jar\n  │       ├── guava-30.0-jre.jar\n  │       ├── postgresql-42.2.14.jar\n  │       └── protobuf-java-3.8.0.jar\n  └── pg-connector.yaml\n  ```\n\n* Deploy the connector configuration:\n\n  ```shell\n  # pwd = .../environment/cdc/\n  oc start-build connect-cluster-connect --from-dir ./my-plugins/ --follow -n ${EVENTSTREAMS_NS}\n  #...\n  # Storing signatures\n  # Successfully pushed image-registry.openshift-image-registry.svc:5000/eventstreams/connect-cluster-connect@sha256:9315b6a6c8f904d0fb5a57f67ba4c9067c6c8264814f283151b20b9d6f147092\n  # Push successful\n  ```\n* Modify the `pg-connector.yaml` from the `environment/cdc` folder to configure the Postgres datasource credentials:\n  ```yaml\n  config: \n    database.dbname: postgres\n    database.hostname: postgres.vaccineorder.svc\n    database.password: pgpwd\n    database.port: 5432\n    database.server.name: vaccine\n    database.user: postgres\n    table.whitelist: public.orderevents\n    plugin.name: pgoutput\n  ```\n* Then start the connector: `oc apply -f pg-connector.yaml -n ${EVENTSTREAMS_NS}`\n* Verify it is running: `oc describe kafkaconnector pg-connector -n ${EVENTSTREAMS_NS}`, you should see one task running. \n* Looking at the pod trace for the connector you should see a successful connection, something like:\n\n```\nSuccessfully tested connection for jdbc:postgresql://postgres.vaccineorder.svc:5432/postgres with user 'postgres' \n```\n* A new topic may have been created with the name of the table replicated: `vaccine.public.orderevents` with new messages mapping the rows in the table.\n\n### Deploy the Optimization Service\n\n## Scenario script\n\nOnce the solution is up and running in your target deployment environment (local or OpenShift) execute the following steps to present an end to end demonstration.\n\n### Validate existing vaccine lot shipment plan\n\nTBD \n\n### Create an order from the user interface of the Order Service. \n\nConnect to the order microservice URL, for example it could look like http://vaccineorderms-vaccine.clusternametochangewithyours.containers.appdomain.cloud. You should reach the home page. Then select Orders tab. \n\nIf there is no order created click on `NEW ORDER` button \n\n ![3](./images/order-ui.png)\n\nand fill the following data:\n\n ```yaml\n organization: Japan Government\n delivery location: Tokyo\n delivery date: 2021-01-24\n quantity: 150\n priority: 2\n vaccine type: covid-19\n ```\n\n\n* Verify the data in postgresql: \n\nOnce submitted the data are saved into the postgres DB, with the outbox pattern having created new records in the corresponding Event tables. \nThis can be done using different tools, like `psql` directly in the pod, or `pgadmin4`.\n\n ```shell\n oc get pods | grep postgres\n oc rsh <postgres pod id>\n # in the shell session within the pod do:\n psql -U postgres\n # in the psql shell, list the table\n \\d \n # Look at the content of the main table:\n select * from public.vaccineorderentity;\n # You should get one new record matching your data\n # Verify outbox pattern works:\n select * from public.orderevents;\n\n id    |   aggregatetype    | aggregateid | type         | timestamp |payload \n c80.. | VaccineOrderEntity | 1           | OrderCreated | 2021-01-24 03:44:08.131634 | {\"orderID\":1,\"deliveryLocation\":\"Tokyo\",\"quantity\":150,\"priority\":2,\"deliveryDate\":\"2021-01-24\",\"askingOrganization\":\"Japan Government\",\"vaccineType\":\"COVID-19\",\"status\":\"OPEN\",\"creationDate\":\"24-Jan-2021 03:44:08\"} | \n ```\n\nThe outbox table of the order events has metadata attributes and then a payload matches the saved record in the orign table.\n\n### Verify the message in the Kafka topic: `vaccine.public.orderevents`\n\n ![5](./images/outbox-topic.png)\n\n  ```json\n  {\"before\":null,\"after\":\n  {\"id\":\"c8050bb4-05d8-4270-833c-083995f27848\",\n   \"aggregatetype\":\"VaccineOrderEntity\",\n   \"aggregateid\":\"1\",\n   \"type\":\"OrderCreated\",\n   \"timestamp\":1611459848131634,\n   \"payload\":\"{\\\"orderID\\\":1,\\\"deliveryLocation\\\":\\\"Paris\\\",\\\"quantity\\\":150,\\\"priority\\\":2,\\\"deliveryDate\\\":\\\"2021-01-24\\\",\\\"askingOrganization\\\":\\\"French Government\\\",\\\"vaccineType\\\":\\\"COVID-19\\\",\\\"status\\\":\\\"OPEN\\\",\\\"creationDate\\\":\\\"24-Jan-2021 03:44:08\\\"}\",\n  \"tracingspancontext\":\"#Sun Jan 24 03:44:08 GMT 2021\\n\"},\n  \"source\":{\"version\":\"1.4.0.Final\",\n  \"connector\":\"postgresql\",\"name\":\"vaccine\",\n  \"ts_ms\":1611712125168,\"snapshot\":\"last\",\n  \"db\":\"postgres\",\"schema\":\"public\",\"table\":\"orderevents\",\n  \"txId\":1371,\"lsn\":41341808,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1611712125173,\"transaction\":null}\n  ```\n\n### REST APIs\n\nThe REST end points this service expose are in the OpenApi doc below, but not all operations are fully implemented yet.\n\n ![4](./images/openapi.png)\n\n\n### See how the shipment plan is modified\n\n","type":"Mdx","contentDigest":"85bba3d1b7182aa0d96640339b147056","counter":324,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Order management and optimization demo","description":"Order management and optimization demonstration"},"exports":{},"rawBody":"---\ntitle: Order management and optimization demo\ndescription: Order management and optimization demonstration\n--- \n\nThe vaccine order fullfilment is a complex problem to address as we need to optimize the shipment plan according to the demand and the contraints. A deep dive to the O/R problem is described in [this note](/solution/orderms/voro-solution/). In this scenario we want to demostrate the order entry with an event-driven microservice, and the integration with an event driven linear programming stateful function. The scenario addresses the following features:\n\n* Create Order(s) using a simple user interface as an order manager will do after interacting with the country health care request.\n* Validate the [transactional outbox pattern](https://ibm-cloud-architecture.github.io/refarch-eda/patterns/intro/#transactional-outbox) works to get OrderCreated, OrderUpdated or OrderCancelled Events created into a dedicated table in Postgresql\n* Validate how [Debezium Change Data Capture](https://debezium.io/documentation/reference/1.4/connectors/postgresql.html) for Postgresql as a Kafka connector, produces OrderEvents from the table to Kafka `orders` topic.\n* Integrate with the [Shipment plan optimization]() to get the updated shipment plan as events.\n\n## Components involved in this use case\n\n* [Vaccine Order Service](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg)\n* Postgres DB\n* Debezium for change data capture on the outbox table\n* Kafka or [event streams](https://ibm.github.io/event-streams/)\n* [Vaccine Order Reefer Optimization Service](https://github.com/ibm-cloud-architecture/vaccine-order-optimizer)\n\n ![](../../solution/orderms/images/vaccine-order-1.png)\n\n*The blockchain integration is not done yet*\n\nTo have a deeper understanding of the order service read [this overview section](/solution/orderms/#overview). For the Vaccine order reefer optimization service [the design article](/design/voro/) details the optimization approach, while the [solution article](/solution/orderms/voro-solution/#overview) goes over the service implementation details.\n\n## Run to OpenShift \n\n### Pre-requisites\n\n* You need:\n    * docker and docker compose on your laptop to run locally or build image\n    * Java 11\n    * git client \n    * Get access to an OpenShift cluster with Cloud Pak for integration and event streams installed.\n1. Create the following artifacts in the `eventstreams` namespace on your OpenShift cluster:\n\n   1. Create an EventStreams instance _(via the [Event Streams Custom Resource](https://ibm.github.io/event-streams/installing/installing/#install-an-event-streams-instance))_.\n   2. Create a [Kafka User with SCRAM-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Reefer Simulator](https://github.com/ibm-cloud-architecture/vaccine-reefer-simulator#application-deployment).\n   3. Create a [Kafka User with TLS-based credentials](https://ibm.github.io/event-streams/security/managing-access/#managing-access-to-kafka-resources), as required by the [Vaccine Monitoring Agent](https://github.com/ibm-cloud-architecture/vaccine-monitoring-agent#create-a-tls-user).\n   4. Create two [Kafka Topics](https://ibm.github.io/event-streams/getting-started/creating-topics/). This tutorial will assume the names of `vaccine.shipment.plans` respectively.\n\n1. Create a new project named `vaccineorder` that will be used for the deployment of all other components.\n1. [OpenShift CLI](https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli) on your local environment.\n1. [jq](https://stedolan.github.io/jq/) on your local environment.\n1. Use a Terminal and the oc cli. If you want to access the code source you can clone the two main repositories of this solution:\n\n ```shell\n git clone https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg\n git clone https://github.com/ibm-cloud-architecture/vaccine-order-optimization-pg\n ```\n\n### Deploy the Vaccine Order Service\n\nThis microservice is built using maven and Quarkus extensions. In this current main project we have \nWe have already pushed the last version of this service on dockerhub, if you do not want to build it. \n\n1. Ensure you are working inside the correct project via the following `oc` command:\n\n  ```shell\n  export PROJECT_NAME=vaccineorder\n  oc project ${PROJECT_NAME}\n  ```\n\n1. Export the value of your Event Streams cluster name into an environment variable:\n\n  ```shell\n  export CLUSTER_NAME=eda-dev\n  export EVENTSTREAMS_NS=eventstreams\n  ```\n   * To check what the name of your Event Streams cluster name is do:\n   ```shell\n   $ oc get eventstreams -n ${EVENTSTREAMS_NS}\n   NAME           STATUS\n   development    Ready\n   ```\n1. Deploy a postgres server. The orders are persisted in an external Postgres instance running on Openshift cluster. To do a simple deployment performs the following commands:\n\n  ```shell\n  # Define environement variables\n  SERVICE_ACCOUNT_NAME=postgres-sa\n  DEPLOYMENT_NAME=postgres\n  SERVICE_NAME=postgres\n  DOCKER_IMAGE=docker.io/postgres:11.6-alpine\n  POSTGRES_PASSWORD=adifficultpasswordtoguess\n\n  oc create serviceaccount ${SERVICE_ACCOUNT_NAME}\n  oc adm policy add-scc-to-user anyuid -n ${PROJECT_NAME} -z ${SERVICE_ACCOUNT_NAME}\n  oc create deployment ${DEPLOYMENT_NAME} --image=${DOCKER_IMAGE}\n  oc set serviceaccount deployment/${DEPLOYMENT_NAME} ${SERVICE_ACCOUNT_NAME}\n  oc patch deployment ${DEPLOYMENT_NAME} --type=\"json\" -p='[{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args\", \"value\":[]},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"-c\"},{\"op\":\"add\", \"path\":\"/spec/template/spec/containers/0/args/-\", \"value\":\"wal_level=logical\"} ]'\n  oc set env deployment ${DEPLOYMENT_NAME} POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n  oc expose deployment ${DEPLOYMENT_NAME} --port 5432 --name ${SERVICE_NAME}\n  ```\n\n1. Get Kafka TLS user name to access Event Streams bootstrap using the external route. export this user name in KAFKA_USER environment variable:\n\n  ```shell\n  oc get kafkausers -n $EVENTSTREAMS_NS \n  # NAME                                CLUSTER   AUTHENTICATION   AUTHORIZATION\n  # app-scram                           eda-dev   scram-sha-512    simple\n  # app-tls                             eda-dev   tls              simple\n  export KAFKA_USER=app-tls\n  ```\n1. Copy Kafka TLS user certificate to target project:\n\n  ```shell\n  oc get secret ${KAFKA_USER} -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n  ```\n1. Get Kafka bootstrap server URL within\n\n   ```shell\n   export KAFKA_BROKERS=$(oc get route -n ${EVENTSTREAMS_NS} ${CLUSTER_NAME}-kafka-bootstrap -o jsonpath=\"{.status.ingress[0].host}:443\")\n   ```\n\n 1. Copy TLS server CA certificate from eventstreams project to local project with the command:\n\n   ```shell\n   oc get secret ${CLUSTER_NAME}-cluster-ca-cert -n ${EVENTSTREAMS_NS} -o json | jq -r '.metadata.name=\"kafka-cluster-ca-cert\"' |jq -r '.metadata.namespace=\"'${PROJECT_NAME}'\"' | oc apply -f -\n   ``` \n\n1. Define environment variables for the order service with config map and secrets:\n\n```shell\n# config map\noc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/configmap.yaml\n# Secrets\noc apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vaccine-order-secrets\nstringData:\n  KAFKA_USER: ${KAFKA_USER}\n  KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_USER}\n  QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}\n  QUARKUS_DATASOURCE_USERNAME: postgres\n```\n\n* Deploy the app. If you have cloned the repository and have java you can use the following command:\n\n ```shell\n ./mvnw clean package -Dui.deps -Dui.dev -Dquarkus.kubernetes.deploy=true -DskipTests\n ```\n\n If not, you can deploy the application using the deployment config as we have pushed the ibmcase/vaccineorders image to dockerhub:\n\n ```shell\noc apply -f  https://raw.githubusercontent.com/ibm-cloud-architecture/vaccine-order-mgr-pg/main/src/main/kubernetes/deploymentconfig.yaml\n\n ```\n\n### Deploy Debezium CDC connector\n\nThe [Event Streams product documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) goes over the tasks to be done to config Kafka Connect cluster, but we can summarize them for our use case as:\n\n* Start a Kafka connector cluster: We use the custom resource definition called `KafkaConnectS2I`, which one instance represents a Kafka connect cluster. Each connector is represented by another custom resource called `KafkaConnector`. Kafka connect needs a user access to the Brokers. We can use the TLS user as previously done (See also [this note](https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/connect-cos/#set-up-the-kafka-connect-cluster) for the same type of setting). \n  * Event Streams UI has a Toolbox menu with the `Set up a Kafka Connect environment` where we can download the `KafkaConnectS2I` configuration. The matching configuration is [in this file](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/blob/main/environment/cdc/kafka-connect-s2i.yaml) and uses predefined TLS user and cluster certificate. The cluster name is `connect-cluster`.\n  * Update the following values in this file: `bootstrapServers` and `secretName: tls`  to `secretName: <yourTLSuser>` and the Server ca certificate secretName like `kafka-cluster-ca-cert`.\n   ```yaml\n   tls:\n    trustedCertificates:\n      - certificate: ca.crt\n        secretName: kafka-cluster-ca-cert\n   ```\n  * Deploy the cluster with: `oc apply -f kafka-connect-s2i.yaml -n ${EVENTSTREAMS_NS}`\n  * Validate it via: \n\n   ```shell\n   oc get kafkaconnects2i -n ${EVENTSTREAMS_NS}\n   oc describe kafkaconnects2i connect-cluster -n ${EVENTSTREAMS_NS}\n   ```\n* Download the postgres plugin archive from [debezium maven repository](https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.4.0.Final/debezium-connector-postgres-1.4.0.Final-plugin.tar.gz) and then add the jar files to the `my-plugins\\debezium-connector` folder. We need a subfolder as this connector includes multiple jars. This step was already done and the debezium connector jars are in the [environment/cdc/my-plugins/debezium-connector)](https://github.com/ibm-cloud-architecture/vaccine-order-mgr-pg/tree/main/environment/cdc/my-plugins/debezium-connector)\n\n  ```\n  ├── my-plugins\n  │   └── debezium-connector\n  │       ├── CHANGELOG.md\n  │       ├── CONTRIBUTE.md\n  │       ├── COPYRIGHT.txt\n  │       ├── LICENSE-3rd-PARTIES.txt\n  │       ├── LICENSE.txt\n  │       ├── README.md\n  │       ├── README_ZH.md\n  │       ├── debezium-api-1.4.0.Final.jar\n  │       ├── debezium-connector-postgres-1.4.0.Final.jar\n  │       ├── debezium-core-1.4.0.Final.jar\n  │       ├── failureaccess-1.0.1.jar\n  │       ├── guava-30.0-jre.jar\n  │       ├── postgresql-42.2.14.jar\n  │       └── protobuf-java-3.8.0.jar\n  └── pg-connector.yaml\n  ```\n\n* Deploy the connector configuration:\n\n  ```shell\n  # pwd = .../environment/cdc/\n  oc start-build connect-cluster-connect --from-dir ./my-plugins/ --follow -n ${EVENTSTREAMS_NS}\n  #...\n  # Storing signatures\n  # Successfully pushed image-registry.openshift-image-registry.svc:5000/eventstreams/connect-cluster-connect@sha256:9315b6a6c8f904d0fb5a57f67ba4c9067c6c8264814f283151b20b9d6f147092\n  # Push successful\n  ```\n* Modify the `pg-connector.yaml` from the `environment/cdc` folder to configure the Postgres datasource credentials:\n  ```yaml\n  config: \n    database.dbname: postgres\n    database.hostname: postgres.vaccineorder.svc\n    database.password: pgpwd\n    database.port: 5432\n    database.server.name: vaccine\n    database.user: postgres\n    table.whitelist: public.orderevents\n    plugin.name: pgoutput\n  ```\n* Then start the connector: `oc apply -f pg-connector.yaml -n ${EVENTSTREAMS_NS}`\n* Verify it is running: `oc describe kafkaconnector pg-connector -n ${EVENTSTREAMS_NS}`, you should see one task running. \n* Looking at the pod trace for the connector you should see a successful connection, something like:\n\n```\nSuccessfully tested connection for jdbc:postgresql://postgres.vaccineorder.svc:5432/postgres with user 'postgres' \n```\n* A new topic may have been created with the name of the table replicated: `vaccine.public.orderevents` with new messages mapping the rows in the table.\n\n### Deploy the Optimization Service\n\n## Scenario script\n\nOnce the solution is up and running in your target deployment environment (local or OpenShift) execute the following steps to present an end to end demonstration.\n\n### Validate existing vaccine lot shipment plan\n\nTBD \n\n### Create an order from the user interface of the Order Service. \n\nConnect to the order microservice URL, for example it could look like http://vaccineorderms-vaccine.clusternametochangewithyours.containers.appdomain.cloud. You should reach the home page. Then select Orders tab. \n\nIf there is no order created click on `NEW ORDER` button \n\n ![3](./images/order-ui.png)\n\nand fill the following data:\n\n ```yaml\n organization: Japan Government\n delivery location: Tokyo\n delivery date: 2021-01-24\n quantity: 150\n priority: 2\n vaccine type: covid-19\n ```\n\n\n* Verify the data in postgresql: \n\nOnce submitted the data are saved into the postgres DB, with the outbox pattern having created new records in the corresponding Event tables. \nThis can be done using different tools, like `psql` directly in the pod, or `pgadmin4`.\n\n ```shell\n oc get pods | grep postgres\n oc rsh <postgres pod id>\n # in the shell session within the pod do:\n psql -U postgres\n # in the psql shell, list the table\n \\d \n # Look at the content of the main table:\n select * from public.vaccineorderentity;\n # You should get one new record matching your data\n # Verify outbox pattern works:\n select * from public.orderevents;\n\n id    |   aggregatetype    | aggregateid | type         | timestamp |payload \n c80.. | VaccineOrderEntity | 1           | OrderCreated | 2021-01-24 03:44:08.131634 | {\"orderID\":1,\"deliveryLocation\":\"Tokyo\",\"quantity\":150,\"priority\":2,\"deliveryDate\":\"2021-01-24\",\"askingOrganization\":\"Japan Government\",\"vaccineType\":\"COVID-19\",\"status\":\"OPEN\",\"creationDate\":\"24-Jan-2021 03:44:08\"} | \n ```\n\nThe outbox table of the order events has metadata attributes and then a payload matches the saved record in the orign table.\n\n### Verify the message in the Kafka topic: `vaccine.public.orderevents`\n\n ![5](./images/outbox-topic.png)\n\n  ```json\n  {\"before\":null,\"after\":\n  {\"id\":\"c8050bb4-05d8-4270-833c-083995f27848\",\n   \"aggregatetype\":\"VaccineOrderEntity\",\n   \"aggregateid\":\"1\",\n   \"type\":\"OrderCreated\",\n   \"timestamp\":1611459848131634,\n   \"payload\":\"{\\\"orderID\\\":1,\\\"deliveryLocation\\\":\\\"Paris\\\",\\\"quantity\\\":150,\\\"priority\\\":2,\\\"deliveryDate\\\":\\\"2021-01-24\\\",\\\"askingOrganization\\\":\\\"French Government\\\",\\\"vaccineType\\\":\\\"COVID-19\\\",\\\"status\\\":\\\"OPEN\\\",\\\"creationDate\\\":\\\"24-Jan-2021 03:44:08\\\"}\",\n  \"tracingspancontext\":\"#Sun Jan 24 03:44:08 GMT 2021\\n\"},\n  \"source\":{\"version\":\"1.4.0.Final\",\n  \"connector\":\"postgresql\",\"name\":\"vaccine\",\n  \"ts_ms\":1611712125168,\"snapshot\":\"last\",\n  \"db\":\"postgres\",\"schema\":\"public\",\"table\":\"orderevents\",\n  \"txId\":1371,\"lsn\":41341808,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1611712125173,\"transaction\":null}\n  ```\n\n### REST APIs\n\nThe REST end points this service expose are in the OpenApi doc below, but not all operations are fully implemented yet.\n\n ![4](./images/openapi.png)\n\n\n### See how the shipment plan is modified\n\n","fileAbsolutePath":"/home/runner/work/vaccine-solution-main/vaccine-solution-main/docs/src/pages/use-cases/order/index.mdx"}}}}